{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arap_2020_06_lm_todo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordivg1/ARAP/blob/main/arap_2020_06_lm_todo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DZwK1xpCd9a"
      },
      "source": [
        "# Language Models\n",
        "\n",
        "**Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OdVfmDx8LoQ"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"You should enable GPU runtime.\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjoZl4ksHQcN"
      },
      "source": [
        "A language model is a probability distribution over sequences of words. To train a Deep Learning language model, we will task the model to, given a sequence of words, predict the following one.\n",
        "\n",
        "For this lab, we will train 2 different models. The first one will be a simple RNN model with a encoder decoder structure. The second one will be a Transformer Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnE3PTxOKxHI"
      },
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZToZpN8aBoO"
      },
      "source": [
        "First we will declare the model that we will use. We will start with a simple RNN model made of an encoder, a recurrent module, and a decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYw3yv_RlO1d"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ9EImaubusi"
      },
      "source": [
        "##### **Exercise 1**\n",
        "Write the forward method of the model, using the encoder layer, the rnn, and the decoder. Note that the forward method should return both the decoded output and the hidden state from the RNN. Use the dropout layer after the encoder layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktBXAZz98N8Y"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, embedding_size, nhid, nlayers, dropout=0.5, pretrained_embeddings=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "        if pretrained_embeddings is None:\n",
        "            self.encoder = nn.Embedding(ntoken, embedding_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(embedding_size, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        if self.pretrained_embeddings is None:\n",
        "          x = self.encoder(x)\n",
        "        else:\n",
        "          x = self.pretrained_embeddings(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        decoded = self.decoder(x)\n",
        "        \n",
        "\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk83j8QUbfGK"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oo-Q2k4FoKq"
      },
      "source": [
        "batch_size = 20\n",
        "bptt = 35  # Back Propagation Through Time\n",
        "embedding_size = 300  # 650 gives better results, but is much slower\n",
        "hidden_size = 300  # 650 gives better results, but is much slower\n",
        "n_layers = 2\n",
        "lr = 1e-2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLpK2vp9blMb"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "For our task we will use the WikiText2 dataset. This language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
        "\n",
        "Starting from sequential data. We will arrange the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "```\n",
        "┌ a g m s ┐\n",
        "│ b h n t │\n",
        "│ c i o u │\n",
        "│ d j p v │\n",
        "│ e k q w │\n",
        "└ f l r x ┘\n",
        "```\n",
        "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient batch processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCsB-ZOgFaA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab691399-f28d-4322-ab76-cb7de227fbfc"
      },
      "source": [
        "dataset = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(dataset)\n",
        "\n",
        "# build the vocabulary\n",
        "dataset.build_vocab(train_txt)\n",
        "ntokens = len(dataset.vocab.stoi)  # stoi = string to int\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.55MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr2J0VHQu9O7"
      },
      "source": [
        "# make iterator for splits\n",
        "train_iter, valid_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
        "    (train_txt, val_txt, test_txt), batch_size=batch_size, bptt_len=bptt, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9MNZgobret"
      },
      "source": [
        "### Instantiate model\n",
        "\n",
        "Here, we instantiate the model and the optimizer. We will also use a LR scheduler to decrease the LR after every epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JhIiMoid8D"
      },
      "source": [
        "#### **Exercise 2**\n",
        "\n",
        "Instantiate the model with the correct hyperparameters. Then, instantiate also the correct loss function for a language model, the Adam optimizer and a [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) learning rate scheduler with step 1 and gamma 0.95."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5uJkYP3G2fp"
      },
      "source": [
        "model = RNNModel(ntokens, embedding_size, hidden_size, n_layers).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixURcUlVb6PP"
      },
      "source": [
        "### Train function\n",
        "\n",
        "Now we define the train function that trains the model for an epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILemaj8gSbI"
      },
      "source": [
        "#### **Exercise 3**\n",
        "\n",
        "Complete the training function with help of the code comments. You can check the documentation of [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOJhvX3zcAwQ"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)  # For LSTMs\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        data, target = batch.text, batch.target\n",
        "\n",
        "        # Set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "\n",
        "        # Compute the output and the new hidden state\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        output = output.permute(0, 2, 1)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # use `clip_grad_norm_` to clip the norm of the gradients to 0.25. It will help the training of the rnn\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 100\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'| epoch {epoch:3d} | {i:5d}/{len(train_iter):5d} batches | lr {lr:.4f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvq79Xn9b8xt"
      },
      "source": [
        "### Validation function\n",
        "\n",
        "Now we will define the validation function, that given a dataset (val or test) will evaluate the loss of the prediction of the model in that dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlg_cNacb3Zy"
      },
      "source": [
        "#### **Exercise 4**\n",
        "\n",
        "Complete the validation function to compute the loss. `data_source` corresponds to `val_data` or `test_data` (depending on which phase of the training code we are).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Bnq5RPcCEl"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    n = 0\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for i, batch in enumerate(data_source):\n",
        "        data, target = batch.text, batch.target\n",
        "        output, hidden = model(data, hidden)\n",
        "        output = output.permute(0, 2, 1)\n",
        "        total_loss += target.numel() * criterion(output, target).item()\n",
        "        n += target.numel()\n",
        "    return total_loss / n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQxGZc2TcEkO"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "This is the training loop code. At any point you can hit stop to get out of training early."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI88vCE7_E5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9c4baf-1aef-4128-88c6-af78136e18a3"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# At any point you can hit stop to get out of training early.\n",
        "epochs = 4\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(valid_iter)\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if val_loss < best_val_loss:\n",
        "            with open(\"best_checkpoint.pth\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        scheduler.step()\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"best_checkpoint.pth\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data. \n",
        "with torch.no_grad():\n",
        "    test_loss = evaluate(test_iter)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n",
        "print('=' * 89)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/ 3196 batches | lr 0.0100 | ms/batch 35.55 | loss  7.35 | ppl  1549.01\n",
            "| epoch   1 |   200/ 3196 batches | lr 0.0100 | ms/batch 28.76 | loss  6.42 | ppl   616.56\n",
            "| epoch   1 |   300/ 3196 batches | lr 0.0100 | ms/batch 28.49 | loss  6.16 | ppl   471.54\n",
            "| epoch   1 |   400/ 3196 batches | lr 0.0100 | ms/batch 28.65 | loss  6.02 | ppl   409.63\n",
            "| epoch   1 |   500/ 3196 batches | lr 0.0100 | ms/batch 28.68 | loss  5.98 | ppl   395.08\n",
            "| epoch   1 |   600/ 3196 batches | lr 0.0100 | ms/batch 28.70 | loss  5.98 | ppl   394.54\n",
            "| epoch   1 |   700/ 3196 batches | lr 0.0100 | ms/batch 28.83 | loss  5.99 | ppl   398.69\n",
            "| epoch   1 |   800/ 3196 batches | lr 0.0100 | ms/batch 28.97 | loss  5.94 | ppl   380.57\n",
            "| epoch   1 |   900/ 3196 batches | lr 0.0100 | ms/batch 28.97 | loss  6.03 | ppl   415.35\n",
            "| epoch   1 |  1000/ 3196 batches | lr 0.0100 | ms/batch 29.00 | loss  5.97 | ppl   392.33\n",
            "| epoch   1 |  1100/ 3196 batches | lr 0.0100 | ms/batch 29.07 | loss  6.04 | ppl   420.70\n",
            "| epoch   1 |  1200/ 3196 batches | lr 0.0100 | ms/batch 29.17 | loss  5.90 | ppl   364.56\n",
            "| epoch   1 |  1300/ 3196 batches | lr 0.0100 | ms/batch 29.26 | loss  5.93 | ppl   375.07\n",
            "| epoch   1 |  1400/ 3196 batches | lr 0.0100 | ms/batch 29.32 | loss  5.85 | ppl   346.66\n",
            "| epoch   1 |  1500/ 3196 batches | lr 0.0100 | ms/batch 29.43 | loss  5.76 | ppl   318.59\n",
            "| epoch   1 |  1600/ 3196 batches | lr 0.0100 | ms/batch 29.56 | loss  5.80 | ppl   331.30\n",
            "| epoch   1 |  1700/ 3196 batches | lr 0.0100 | ms/batch 29.56 | loss  5.86 | ppl   350.17\n",
            "| epoch   1 |  1800/ 3196 batches | lr 0.0100 | ms/batch 29.63 | loss  5.81 | ppl   332.32\n",
            "| epoch   1 |  1900/ 3196 batches | lr 0.0100 | ms/batch 29.73 | loss  5.86 | ppl   349.97\n",
            "| epoch   1 |  2000/ 3196 batches | lr 0.0100 | ms/batch 29.84 | loss  5.83 | ppl   339.63\n",
            "| epoch   1 |  2100/ 3196 batches | lr 0.0100 | ms/batch 29.87 | loss  5.75 | ppl   314.38\n",
            "| epoch   1 |  2200/ 3196 batches | lr 0.0100 | ms/batch 30.08 | loss  5.78 | ppl   323.90\n",
            "| epoch   1 |  2300/ 3196 batches | lr 0.0100 | ms/batch 30.11 | loss  5.70 | ppl   297.75\n",
            "| epoch   1 |  2400/ 3196 batches | lr 0.0100 | ms/batch 30.39 | loss  5.63 | ppl   278.79\n",
            "| epoch   1 |  2500/ 3196 batches | lr 0.0100 | ms/batch 30.43 | loss  5.77 | ppl   321.66\n",
            "| epoch   1 |  2600/ 3196 batches | lr 0.0100 | ms/batch 30.41 | loss  5.69 | ppl   296.29\n",
            "| epoch   1 |  2700/ 3196 batches | lr 0.0100 | ms/batch 30.41 | loss  5.72 | ppl   304.95\n",
            "| epoch   1 |  2800/ 3196 batches | lr 0.0100 | ms/batch 30.42 | loss  5.69 | ppl   295.10\n",
            "| epoch   1 |  2900/ 3196 batches | lr 0.0100 | ms/batch 30.45 | loss  5.59 | ppl   266.69\n",
            "| epoch   1 |  3000/ 3196 batches | lr 0.0100 | ms/batch 30.43 | loss  5.63 | ppl   279.46\n",
            "| epoch   1 |  3100/ 3196 batches | lr 0.0100 | ms/batch 30.43 | loss  5.65 | ppl   283.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 1 | time: 98.35s | valid loss 5.19 | valid ppl 178.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/ 3196 batches | lr 0.0095 | ms/batch 34.53 | loss  5.49 | ppl   241.26\n",
            "| epoch   2 |   200/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.46 | ppl   233.96\n",
            "| epoch   2 |   300/ 3196 batches | lr 0.0095 | ms/batch 30.50 | loss  5.44 | ppl   230.29\n",
            "| epoch   2 |   400/ 3196 batches | lr 0.0095 | ms/batch 30.64 | loss  5.43 | ppl   229.29\n",
            "| epoch   2 |   500/ 3196 batches | lr 0.0095 | ms/batch 30.69 | loss  5.35 | ppl   209.65\n",
            "| epoch   2 |   600/ 3196 batches | lr 0.0095 | ms/batch 30.81 | loss  5.33 | ppl   205.95\n",
            "| epoch   2 |   700/ 3196 batches | lr 0.0095 | ms/batch 30.92 | loss  5.36 | ppl   212.67\n",
            "| epoch   2 |   800/ 3196 batches | lr 0.0095 | ms/batch 30.90 | loss  5.31 | ppl   203.06\n",
            "| epoch   2 |   900/ 3196 batches | lr 0.0095 | ms/batch 30.82 | loss  5.41 | ppl   224.26\n",
            "| epoch   2 |  1000/ 3196 batches | lr 0.0095 | ms/batch 30.71 | loss  5.41 | ppl   222.98\n",
            "| epoch   2 |  1100/ 3196 batches | lr 0.0095 | ms/batch 30.61 | loss  5.46 | ppl   235.31\n",
            "| epoch   2 |  1200/ 3196 batches | lr 0.0095 | ms/batch 30.50 | loss  5.35 | ppl   210.64\n",
            "| epoch   2 |  1300/ 3196 batches | lr 0.0095 | ms/batch 30.41 | loss  5.42 | ppl   225.84\n",
            "| epoch   2 |  1400/ 3196 batches | lr 0.0095 | ms/batch 30.39 | loss  5.38 | ppl   216.64\n",
            "| epoch   2 |  1500/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.28 | ppl   196.09\n",
            "| epoch   2 |  1600/ 3196 batches | lr 0.0095 | ms/batch 30.39 | loss  5.36 | ppl   212.53\n",
            "| epoch   2 |  1700/ 3196 batches | lr 0.0095 | ms/batch 30.39 | loss  5.43 | ppl   227.07\n",
            "| epoch   2 |  1800/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.40 | ppl   220.57\n",
            "| epoch   2 |  1900/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.44 | ppl   231.50\n",
            "| epoch   2 |  2000/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.43 | ppl   227.76\n",
            "| epoch   2 |  2100/ 3196 batches | lr 0.0095 | ms/batch 30.42 | loss  5.37 | ppl   215.79\n",
            "| epoch   2 |  2200/ 3196 batches | lr 0.0095 | ms/batch 30.41 | loss  5.40 | ppl   221.47\n",
            "| epoch   2 |  2300/ 3196 batches | lr 0.0095 | ms/batch 30.40 | loss  5.28 | ppl   196.40\n",
            "| epoch   2 |  2400/ 3196 batches | lr 0.0095 | ms/batch 30.39 | loss  5.29 | ppl   197.90\n",
            "| epoch   2 |  2500/ 3196 batches | lr 0.0095 | ms/batch 30.41 | loss  5.41 | ppl   224.24\n",
            "| epoch   2 |  2600/ 3196 batches | lr 0.0095 | ms/batch 30.43 | loss  5.37 | ppl   215.10\n",
            "| epoch   2 |  2700/ 3196 batches | lr 0.0095 | ms/batch 30.61 | loss  5.40 | ppl   220.57\n",
            "| epoch   2 |  2800/ 3196 batches | lr 0.0095 | ms/batch 30.57 | loss  5.37 | ppl   214.04\n",
            "| epoch   2 |  2900/ 3196 batches | lr 0.0095 | ms/batch 30.67 | loss  5.27 | ppl   194.73\n",
            "| epoch   2 |  3000/ 3196 batches | lr 0.0095 | ms/batch 30.67 | loss  5.34 | ppl   207.95\n",
            "| epoch   2 |  3100/ 3196 batches | lr 0.0095 | ms/batch 30.66 | loss  5.35 | ppl   210.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 2 | time: 101.24s | valid loss 5.11 | valid ppl 166.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/ 3196 batches | lr 0.0090 | ms/batch 34.69 | loss  5.32 | ppl   204.89\n",
            "| epoch   3 |   200/ 3196 batches | lr 0.0090 | ms/batch 30.37 | loss  5.20 | ppl   182.16\n",
            "| epoch   3 |   300/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.33 | ppl   206.31\n",
            "| epoch   3 |   400/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.21 | ppl   183.58\n",
            "| epoch   3 |   500/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.15 | ppl   172.09\n",
            "| epoch   3 |   600/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.15 | ppl   173.16\n",
            "| epoch   3 |   700/ 3196 batches | lr 0.0090 | ms/batch 30.40 | loss  5.17 | ppl   175.26\n",
            "| epoch   3 |   800/ 3196 batches | lr 0.0090 | ms/batch 30.40 | loss  5.15 | ppl   171.59\n",
            "| epoch   3 |   900/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.27 | ppl   193.85\n",
            "| epoch   3 |  1000/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.25 | ppl   190.41\n",
            "| epoch   3 |  1100/ 3196 batches | lr 0.0090 | ms/batch 30.37 | loss  5.32 | ppl   204.61\n",
            "| epoch   3 |  1200/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.23 | ppl   186.38\n",
            "| epoch   3 |  1300/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.28 | ppl   196.36\n",
            "| epoch   3 |  1400/ 3196 batches | lr 0.0090 | ms/batch 30.37 | loss  5.23 | ppl   186.32\n",
            "| epoch   3 |  1500/ 3196 batches | lr 0.0090 | ms/batch 30.42 | loss  5.11 | ppl   166.49\n",
            "| epoch   3 |  1600/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.19 | ppl   180.19\n",
            "| epoch   3 |  1700/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.27 | ppl   194.32\n",
            "| epoch   3 |  1800/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.23 | ppl   186.37\n",
            "| epoch   3 |  1900/ 3196 batches | lr 0.0090 | ms/batch 30.37 | loss  5.28 | ppl   196.32\n",
            "| epoch   3 |  2000/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.28 | ppl   195.87\n",
            "| epoch   3 |  2100/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.22 | ppl   184.70\n",
            "| epoch   3 |  2200/ 3196 batches | lr 0.0090 | ms/batch 30.40 | loss  5.26 | ppl   192.32\n",
            "| epoch   3 |  2300/ 3196 batches | lr 0.0090 | ms/batch 30.40 | loss  5.17 | ppl   175.09\n",
            "| epoch   3 |  2400/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.14 | ppl   169.88\n",
            "| epoch   3 |  2500/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.25 | ppl   191.38\n",
            "| epoch   3 |  2600/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.22 | ppl   184.84\n",
            "| epoch   3 |  2700/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.23 | ppl   186.86\n",
            "| epoch   3 |  2800/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.23 | ppl   185.96\n",
            "| epoch   3 |  2900/ 3196 batches | lr 0.0090 | ms/batch 30.39 | loss  5.13 | ppl   169.64\n",
            "| epoch   3 |  3000/ 3196 batches | lr 0.0090 | ms/batch 30.38 | loss  5.18 | ppl   177.23\n",
            "| epoch   3 |  3100/ 3196 batches | lr 0.0090 | ms/batch 30.37 | loss  5.20 | ppl   180.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 3 | time: 100.74s | valid loss 5.14 | valid ppl 170.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/ 3196 batches | lr 0.0086 | ms/batch 34.81 | loss  5.16 | ppl   174.99\n",
            "| epoch   4 |   200/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.07 | ppl   159.82\n",
            "| epoch   4 |   300/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.14 | ppl   170.84\n",
            "| epoch   4 |   400/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.13 | ppl   169.03\n",
            "| epoch   4 |   500/ 3196 batches | lr 0.0086 | ms/batch 30.41 | loss  5.08 | ppl   160.75\n",
            "| epoch   4 |   600/ 3196 batches | lr 0.0086 | ms/batch 30.41 | loss  5.04 | ppl   154.06\n",
            "| epoch   4 |   700/ 3196 batches | lr 0.0086 | ms/batch 30.50 | loss  5.04 | ppl   154.23\n",
            "| epoch   4 |   800/ 3196 batches | lr 0.0086 | ms/batch 30.58 | loss  5.05 | ppl   155.98\n",
            "| epoch   4 |   900/ 3196 batches | lr 0.0086 | ms/batch 30.38 | loss  5.17 | ppl   175.14\n",
            "| epoch   4 |  1000/ 3196 batches | lr 0.0086 | ms/batch 30.49 | loss  5.15 | ppl   172.76\n",
            "| epoch   4 |  1100/ 3196 batches | lr 0.0086 | ms/batch 30.49 | loss  5.23 | ppl   186.12\n",
            "| epoch   4 |  1200/ 3196 batches | lr 0.0086 | ms/batch 30.48 | loss  5.14 | ppl   169.95\n",
            "| epoch   4 |  1300/ 3196 batches | lr 0.0086 | ms/batch 30.38 | loss  5.19 | ppl   179.86\n",
            "| epoch   4 |  1400/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.13 | ppl   169.17\n",
            "| epoch   4 |  1500/ 3196 batches | lr 0.0086 | ms/batch 30.48 | loss  5.02 | ppl   151.58\n",
            "| epoch   4 |  1600/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.11 | ppl   165.52\n",
            "| epoch   4 |  1700/ 3196 batches | lr 0.0086 | ms/batch 30.48 | loss  5.17 | ppl   175.37\n",
            "| epoch   4 |  1800/ 3196 batches | lr 0.0086 | ms/batch 30.41 | loss  5.13 | ppl   169.63\n",
            "| epoch   4 |  1900/ 3196 batches | lr 0.0086 | ms/batch 30.41 | loss  5.21 | ppl   182.52\n",
            "| epoch   4 |  2000/ 3196 batches | lr 0.0086 | ms/batch 30.39 | loss  5.19 | ppl   179.83\n",
            "| epoch   4 |  2100/ 3196 batches | lr 0.0086 | ms/batch 30.45 | loss  5.13 | ppl   168.45\n",
            "| epoch   4 |  2200/ 3196 batches | lr 0.0086 | ms/batch 30.42 | loss  5.18 | ppl   177.71\n",
            "| epoch   4 |  2300/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.05 | ppl   155.71\n",
            "| epoch   4 |  2400/ 3196 batches | lr 0.0086 | ms/batch 30.49 | loss  5.03 | ppl   152.43\n",
            "| epoch   4 |  2500/ 3196 batches | lr 0.0086 | ms/batch 30.52 | loss  5.17 | ppl   175.54\n",
            "| epoch   4 |  2600/ 3196 batches | lr 0.0086 | ms/batch 30.42 | loss  5.12 | ppl   168.09\n",
            "| epoch   4 |  2700/ 3196 batches | lr 0.0086 | ms/batch 30.39 | loss  5.15 | ppl   172.15\n",
            "| epoch   4 |  2800/ 3196 batches | lr 0.0086 | ms/batch 30.43 | loss  5.11 | ppl   165.10\n",
            "| epoch   4 |  2900/ 3196 batches | lr 0.0086 | ms/batch 30.40 | loss  5.02 | ppl   151.20\n",
            "| epoch   4 |  3000/ 3196 batches | lr 0.0086 | ms/batch 30.47 | loss  5.11 | ppl   165.67\n",
            "| epoch   4 |  3100/ 3196 batches | lr 0.0086 | ms/batch 30.47 | loss  5.11 | ppl   165.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 4 | time: 100.92s | valid loss 5.16 | valid ppl 174.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.97 | test ppl   143.53\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EamPZGYGcd1_"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghEye4mBJmCZ"
      },
      "source": [
        "Now we can test the performance of our language model, by first inputting a random word to the model, generating a new word (by taking the most likely output from the model) and then inputting the generated word to the model iteratively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzKMoYJT_rJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df936a9f-bb87-4c86-938b-20f9eda2ef5c"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "hidden = model.init_hidden(bsz=1)\n",
        "x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "temperature = 1  # Higher will increase diversity\n",
        "text = \"\"\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        output, hidden = model(x, hidden)\n",
        "        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        x = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n",
        "        word = dataset.vocab.itos[word_idx]\n",
        "        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "james 41 ( 2 6 ) in history 's hospital sulfide @-@ turkish , 1918 passed and lbw per kombat\n",
            "queen forces and eukaryotes batters , in monday to read a telephone @-@ placed . <eos>\n",
            "  <eos>\n",
            "  =\n",
            "= impact = = <eos>\n",
            "  <eos>\n",
            "  according to jeremy : almost application returned to foreign bow , players\n",
            ": common lights petty through a touchdown of the order city . <eos>\n",
            "  in < unk > william scholar\n",
            "daughter out was estimated in the lucky , until the business islands , ross pulteney ( < unk > )\n",
            ", anti @-@ shaped two ( compassion robinson behavior , wounding < unk > and bethany per the release .\n",
            "and awareness regiment ) and was asked . the north of the discrepancies also it is the end of loyalty\n",
            "up on the island , and completed alabama porter . all of march , and gordon was optimistic after viii\n",
            "( < unk > < unk > < unk > ) is en moderately @-@ ranked douglas luck europe a\n",
            "150 @-@ region earlier being more 55 , allowing introduced or in what the @-@ levels of 69 and celtic\n",
            "fixed yards , which missed the agreement version of large clair than 32 knots . after the all who defends\n",
            "an eye , 2010 's center , a modern . as sweet on < unk > administration between total chivo\n",
            "from former 270 and the pass with death as a major area of machismo , cambridge co @-@ yard and\n",
            "at force somaliland . only the storyline , bathing had previously 5 @,@ 000 @.@ 28 @.@ 0 µm ,\n",
            "when the majors is first discovered to freely the final french surgery of the ground . if actual english ships\n",
            ", ronald would reach the evil marriage and falsetto massive theatres , but the score of 9 with 17 billion\n",
            "when keyboards is considerable admits , and critics a certain @-@ time 's authority , and 2007 marched on northern\n",
            "13 lead and capable of some game in one @-@ time live @-@ discipline severed alden coast ; the fia\n",
            "brian won phoebe universe ever last program and in the world edition , who researched again the wonderful promoted pass\n",
            "to the broncos monument . the crew was in each town might record in october 1903 , and recently named\n",
            "journal have been inducted \" as the couple following irregular to people destroying their shoulders from te ridge . barry\n",
            "reviewer followed his previous scoreless asteroid in the state olympics to appoint ... the actual election , killed the report\n",
            "and a mixed company in march cases . almost episode on the field on june 31 , 1946 . 24\n",
            "tech b. venerated louisiana are like , who set captain other molecules or < unk > . at a number\n",
            "of exhaustion span during the first new year , prpić was met by dissolving 1911 . the us ( welsh\n",
            "– yard @.@ 27 rating ) secret no books from music its christians in the front of 32 may <\n",
            "unk > of wales clement , 1994 for day , distortions announced that lasted from flower 's rights supply on\n",
            "2 – 2 win . the point 's autopsy were just to the twins at route poisoning ( 66 –\n",
            "man in the play area by walpole . <eos>\n",
            "  <eos>\n",
            "  = = = < unk > and =\n",
            "= = <eos>\n",
            "  <eos>\n",
            "  but marked many total temperature , not actively moored one 1 @.@ 1 million\n",
            "to 9 tuff 44 victory and the bruins . <eos>\n",
            "  <eos>\n",
            "  = = = = testing and tech\n",
            "= = = <eos>\n",
            "  <eos>\n",
            "  fez had found claude birds in the peach , as well as sr\n",
            "than 24 % as follows time , which hall was the character and gave his tour board , and sung\n",
            "in january 1949 . he claimed also classified as the planet like out run by kenneth midfield . m. males\n",
            "was named to enhance . <eos>\n",
            "  during the son also choose for rangers , double — academy , with\n",
            "their time kicked in mozambique . <eos>\n",
            "  in gregory , the irish translation of the capital — cbs emerged\n",
            "— trustees was typically in the list of departure of woman . park are organised by january against the proper\n",
            "year concerning a downloadable committees . <eos>\n",
            "  ceres has warranted four – 3 million , chance was written into\n",
            "with a fia < unk > ' malley party ? ( 1972 ) , and in 1983 as an detected\n",
            "secretary of a sea barbettes . even co @-@ and him won praise fantasy is me by authority that the\n",
            "song 's intro of son in the secret their finkelstein . ono work of the 1830 was undertook once that\n",
            "this was replaced by her burgeoning birds to take to visit ross territory in 1821 , which at 27 mm\n",
            ". at cbs utc on the spores again safety followed that \" theatre 's < unk > to the offense\n",
            "had been are found , but jesse fewer years 45 @-@ yard barrier ( riaa ) , with dublin and\n",
            "jewelled 's first down for the goddess had been denied by no concept . on 9 years and grandfather complete\n",
            "sometimes directly with the pitcher of the units of dublin , have entered moderate slow . pitkern possessed that pratt\n",
            "< unk > on one ridge in english ireland in june india to < unk > delivered the references ,\n",
            "scotti had traded with manuscript of the discovery to read < unk > , and uses west . when the\n",
            "king has been hollow at a version of india ranger sites their neglect in ethiopia from the reviews . <eos>\n",
            "  there is some freedom of august 20 deaths of cubic but cbs succeeded he testing her first h. as\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppSA30_FyJeS"
      },
      "source": [
        "# Use pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA3jdPbRxCXr"
      },
      "source": [
        "dataset = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(dataset)\n",
        "\n",
        "# build the vocabulary\n",
        "embeddings = torchtext.vocab.GloVe(name='6B', dim=300)\n",
        "dataset.build_vocab(train_txt, vectors=embeddings)  # Specify the embedding https://nlp.stanford.edu/projects/glove/\n",
        "ntokens = len(dataset.vocab.stoi)  # stoi = string to int\n",
        "print(dataset.vocab.vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7DXC43XxkxW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1678b69d-22a8-44a7-f747-28f466cc12be"
      },
      "source": [
        "def get_embedding(word):\n",
        "    index = dataset.vocab.stoi[word]\n",
        "    return dataset.vocab.vectors[index]\n",
        "\n",
        "print(get_embedding(\"queen\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a11c07a59fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"queen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-a11c07a59fd9>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"queen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVUkUaaDhEV5"
      },
      "source": [
        "#### **Exercise 5**\n",
        "\n",
        "Complete the code to find the closest embeddings to `queen` - `woman` + `man`. You can use `torch.dist` to compute the distance between 2 vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc-xkHiW1Qhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "63bffa8d-cc5e-44b0-b070-9c79c4766bbf"
      },
      "source": [
        "embedding = get_embedding(\"queen\") - get_embedding(\"woman\") + get_embedding(\"man\")\n",
        "\n",
        "distances = []\n",
        "for i, vector in enumerate(dataset.vocab.vectors):\n",
        "    if dataset.vocab.stoi[\"queen\"] != i:\n",
        "        distance = torch.dist(embedding, vector)\n",
        "        distances.append(distance)\n",
        "    else:\n",
        "        distances.append(torch.tensor(float(\"inf\")))\n",
        "distances = torch.stack(distances)\n",
        "indices = torch.topk(-distances, k=5)[1]\n",
        "print([dataset.vocab.itos[ind] for ind in indices])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-eaa5201bdf73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"queen\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"woman\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"man\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"queen\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_embedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0FAZkVYhZhq"
      },
      "source": [
        "#### **Exercise 6**\n",
        "\n",
        "Feel free to try on your own to see what embeddings are close to each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6pbsRgH3-ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZNy_qcM6abP"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9zTHr4y6ZfY"
      },
      "source": [
        "model = RNNModel(ntokens, embedding_size, hidden_size, n_layers, pretrained_embeddings=dataset.vocab.vectors.to(device)).to(device)\n",
        "lr = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1JBh42R7c4e"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fucVUohz7ceU"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# At any point you can hit stop to get out of training early.\n",
        "epochs = 4\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(valid_iter)\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if val_loss < best_val_loss:\n",
        "            with open(\"best_checkpoint.pth\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        scheduler.step()\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"best_checkpoint.pth\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data. \n",
        "with torch.no_grad():\n",
        "    test_loss = evaluate(test_iter)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n",
        "print('=' * 89)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4NABKtdfj7"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG9bZ4CCdhDz"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "hidden = model.init_hidden(bsz=1)\n",
        "x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "temperature = 1  # Higher will increase diversity\n",
        "text = \"\"\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        output, hidden = model(x, hidden)\n",
        "        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        x = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n",
        "        word = dataset.vocab.itos[word_idx]\n",
        "        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0NQrOFgK1gd"
      },
      "source": [
        "# Extra: Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ppcvdgRiLX"
      },
      "source": [
        "Now we will train a Transformer to solve the language modelling task. The structure of the architecture of the model is the following:\n",
        "\n",
        "![alt text](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnbNdWveLJIP"
      },
      "source": [
        "Even though it seems complicated, with PyTorch and the `nn.TransformerEncoder` module this can be implemented in an easy (or at least, easier) way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dJgJSNtcoaV"
      },
      "source": [
        "### Positional Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqcu6ov_M6iw"
      },
      "source": [
        "First, we will use a Positional Encoding module. Positional Encoding injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVbG3Va5M1ug"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFXvAySlxnq"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-kI_roTKgix"
      },
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.ninp = ninp\n",
        "        self.src_mask = None\n",
        "\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        \n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)  # Lower triangular matrix with ones.\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = x.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(x):\n",
        "                mask = self._generate_square_subsequent_mask(len(x)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        embeddings = self.encoder(x)\n",
        "        embeddings = embeddings * math.sqrt(self.ninp)\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "\n",
        "        encoded = self.transformer_encoder(embeddings, self.src_mask)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CT8eIecvKF"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbAXoqQCOIKq"
      },
      "source": [
        "batch_size = 20\n",
        "bptt = 35  # Back Propagation Through Time\n",
        "embedding_size = 300  # 650 gives better results, but is much slower\n",
        "hidden_size = 300  # 650 gives better results, but is much slower\n",
        "n_layers = 2\n",
        "n_heads = 2  # Transformer heads\n",
        "lr = 1e-3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYwbgmMbdGJ9"
      },
      "source": [
        "### Model\n",
        "Here, we instantiate the model and the optimizer. We will also use a LR scheduler to decrease the LR after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBoxXR8ADucK"
      },
      "source": [
        "model = TransformerModel(ntokens, embedding_size, n_heads, hidden_size, n_layers).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59HqXgXudJXq"
      },
      "source": [
        "### Train function\n",
        "Now we define the train function that trains the model for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-Co7ggY-aw"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        data, target = batch.text, batch.target\n",
        "        model.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.permute(0, 2, 1)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 100\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'| epoch {epoch:3d} | {i:5d}/{len(train_iter):5d} batches | lr {lr:.4f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg4MBanadLww"
      },
      "source": [
        "### Validation function\n",
        "\n",
        "Now we will define the validation function, that given a dataset (val or test) will evaluate the loss of the prediction of the model in that dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUu9VNhzZBEU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    n = 0\n",
        "    for i, batch in enumerate(data_source):\n",
        "        data, target = batch.text, batch.target\n",
        "        output = model(data)\n",
        "        output = output.permute(0, 2, 1)\n",
        "        total_loss += target.numel() * criterion(output, target).item()\n",
        "        n += target.numel()\n",
        "    return total_loss / n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKtCwcO8dOxA"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "This is the training loop code. At any point you can hit stop to get out of training early."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5snBY1x7OdHw"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# At any point you can hit stop to get out of training early.\n",
        "epochs = 4\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(valid_iter)\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if val_loss < best_val_loss:\n",
        "            with open(\"best_checkpoint_transformer.pth\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        scheduler.step()\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"best_checkpoint_transformer.pth\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "\n",
        "# Run on test data. \n",
        "with torch.no_grad():\n",
        "    test_loss = evaluate(test_iter)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n",
        "print('=' * 89)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh17YMwgl1ei"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kxNIbY2PMgz"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "temperature = 1  # Higher will increase diversity\n",
        "text = \"\"\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        output = model(x)\n",
        "        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        word_tensor = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n",
        "        x = torch.cat([x, word_tensor], dim=1)\n",
        "        x = x[:, -35:]\n",
        "        word = dataset.vocab.itos[word_idx]\n",
        "        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIMWPxpRrhf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}